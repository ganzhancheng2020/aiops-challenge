{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d5f0b2-f1f1-4157-ac20-0d776deb901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataFilters\n",
    "from llama_index.core.indices.query.query_transform.base import (\n",
    "    StepDecomposeQueryTransform,\n",
    ")\n",
    "from llama_index.core.query_engine.multistep_query_engine import MultiStepQueryEngine\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "\n",
    "def get_perform_query(vector_index, query_content, current_keyword):\n",
    "    # 更新查询引擎的过滤条件\n",
    "    value = d[current_keyword]\n",
    "    index_summary = f\"用于回答关于中兴通讯网络运维中{value}的问题\"\n",
    "    step_decompose_transform = StepDecomposeQueryTransform(verbose=True)\n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        filters=MetadataFilters(\n",
    "            filters=[{\"key\": \"product_name\",\n",
    "                      \"value\": current_keyword}]\n",
    "        ),\n",
    "        text_qa_template=text_qa_template,\n",
    "        refine_template=refine_template,\n",
    "        similarity_top_k=3\n",
    "    )\n",
    "    # hyde = HyDEQueryTransform(include_original=True)\n",
    "    # query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "\n",
    "    # MultiStepQuery = MultiStepQueryEngine(\n",
    "    #      query_engine=query_engine,\n",
    "    #      query_transform=step_decompose_transform,\n",
    "    #      index_summary=index_summary,\n",
    "         \n",
    "    # )\n",
    "    # # 执行查询\n",
    "    # response = MultiStepQuery.query(query_content)\n",
    "    # if response.response == 'Empty Response':\n",
    "    \n",
    "    return query_engine\n",
    "    \n",
    "# from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "# from llama_index.node_parser import SentenceWindowNodeParser\n",
    "# from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "# from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "# from llama_index import load_index_from_storage\n",
    "# import os\n",
    "\n",
    "# def build_sentence_window_index(\n",
    "#     document, llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", save_dir=\"sentence_index\"\n",
    "# ):\n",
    "#     # create the sentence window node parser w/ default settings\n",
    "#     node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "#         window_size=3,\n",
    "#         window_metadata_key=\"window\",\n",
    "#         original_text_metadata_key=\"original_text\",\n",
    "#     )\n",
    "#     sentence_context = ServiceContext.from_defaults(\n",
    "#         llm=llm,\n",
    "#         embed_model=embed_model,\n",
    "#         node_parser=node_parser,\n",
    "#     )\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         sentence_index = VectorStoreIndex.from_documents(\n",
    "#             [document], service_context=sentence_context\n",
    "#         )\n",
    "#         sentence_index.storage_context.persist(persist_dir=save_dir)\n",
    "#     else:\n",
    "#         sentence_index = load_index_from_storage(\n",
    "#             StorageContext.from_defaults(persist_dir=save_dir),\n",
    "#             service_context=sentence_context,\n",
    "#         )\n",
    "\n",
    "#     return sentence_index\n",
    "\n",
    "# def get_sentence_window_query_engine(\n",
    "#     sentence_index,\n",
    "#     similarity_top_k=6,\n",
    "#     rerank_top_n=2,\n",
    "# ):\n",
    "#     # define postprocessors\n",
    "#     postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "#     rerank = SentenceTransformerRerank(\n",
    "#         top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "#     )\n",
    "\n",
    "#     sentence_window_engine = sentence_index.as_query_engine(\n",
    "#         similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "#     )\n",
    "#     return sentence_window_engine\n",
    "\n",
    "# def build_automerging_index(\n",
    "#     documents,\n",
    "#     llm,\n",
    "#     embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "#     save_dir=\"merging_index\",\n",
    "#     chunk_sizes=None,\n",
    "# ):\n",
    "#     chunk_sizes = chunk_sizes or [2048, 512, 128]\n",
    "#     node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=chunk_sizes)\n",
    "#     nodes = node_parser.get_nodes_from_documents(documents)\n",
    "#     leaf_nodes = get_leaf_nodes(nodes)\n",
    "#     merging_context = ServiceContext.from_defaults(\n",
    "#         llm=llm,\n",
    "#         embed_model=embed_model,\n",
    "#     )\n",
    "#     storage_context = StorageContext.from_defaults()\n",
    "#     storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         automerging_index = VectorStoreIndex(\n",
    "#             leaf_nodes, storage_context=storage_context, service_context=merging_context\n",
    "#         )\n",
    "#         automerging_index.storage_context.persist(persist_dir=save_dir)\n",
    "#     else:\n",
    "#         automerging_index = load_index_from_storage(\n",
    "#             StorageContext.from_defaults(persist_dir=save_dir),\n",
    "#             service_context=merging_context,\n",
    "#         )\n",
    "#     return automerging_index\n",
    "\n",
    "# from llama_index.node_parser import HierarchicalNodeParser\n",
    "\n",
    "# from llama_index.node_parser import get_leaf_nodes\n",
    "# from llama_index import StorageContext\n",
    "# from llama_index.retrievers import AutoMergingRetriever\n",
    "# from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "# from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "\n",
    "# def get_automerging_query_engine(\n",
    "#     automerging_index,\n",
    "#     similarity_top_k=12,\n",
    "#     rerank_top_n=2,\n",
    "# ):\n",
    "#     base_retriever = automerging_index.as_retriever(similarity_top_k=similarity_top_k)\n",
    "#     retriever = AutoMergingRetriever(\n",
    "#         base_retriever, automerging_index.storage_context, verbose=True\n",
    "#     )\n",
    "#     rerank = SentenceTransformerRerank(\n",
    "#         top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "#     )\n",
    "#     auto_merging_engine = RetrieverQueryEngine.from_args(\n",
    "#         retriever, node_postprocessors=[rerank]\n",
    "#     )\n",
    "#     return auto_merging_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df6c91-709f-42bb-bf14-bf8b9af245d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde5781-c197-4b33-9891-6d1015802791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
